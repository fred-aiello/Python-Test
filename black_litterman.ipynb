{"metadata":{"date":1663034799.5534818,"filename":"black_litterman.md","kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"title":"Two Modifications of Mean-Variance Portfolio Theory","language_info":{"name":"python","version":"3.8.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://investmentanalytixs.com/index.php/2022/05/12/portfolio-optimization-of-using-variance-covariance-method-in-python/\n\nhttps://reasonabledeviations.com/2020/01/04/black-litterman-algotrading/","metadata":{},"id":"1ed561c2-07d5-4853-b50d-d30d02469a02"},{"cell_type":"markdown","source":"\n<a id='black-litterman'></a>\n\n<a id='index-0'></a>","metadata":{},"id":"1b0b1b1b"},{"cell_type":"markdown","source":"# Two Modifications of Mean-Variance Portfolio Theory","metadata":{},"id":"d7a84a7f"},{"cell_type":"markdown","source":"## Contents\n\n- [Two Modifications of Mean-Variance Portfolio Theory](#Two-Modifications-of-Mean-Variance-Portfolio-Theory)  \n  - [Overview](#Overview)  \n  - [Mean-Variance Portfolio Choice](#Mean-Variance-Portfolio-Choice)  \n  - [Estimating  Mean and Variance](#Estimating--Mean-and-Variance)  \n  - [Black-Litterman Starting Point](#Black-Litterman-Starting-Point)  \n  - [Details](#Details)  \n  - [Adding Views](#Adding-Views)  \n  - [Bayesian Interpretation](#Bayesian-Interpretation)  \n  - [Curve Decolletage](#Curve-Decolletage)  \n  - [Black-Litterman Recommendation as Regularization](#Black-Litterman-Recommendation-as-Regularization)  \n  - [A Robust Control Operator](#A-Robust-Control-Operator)  \n  - [A Robust Mean-Variance Portfolio Model](#A-Robust-Mean-Variance-Portfolio-Model)  \n  - [Appendix](#Appendix)  \n  - [Special Case – IID Sample](#Special-Case-–-IID-Sample)  \n  - [Dependence and Sampling Frequency](#Dependence-and-Sampling-Frequency)  \n  - [Frequency and the Mean Estimator](#Frequency-and-the-Mean-Estimator)  ","metadata":{},"id":"40d7fe37"},{"cell_type":"markdown","source":"## Overview\n\nThis lecture describes extensions to the classical mean-variance portfolio theory summarized in our lecture [Elementary Asset Pricing Theory](https://python-advanced.quantecon.org/asset_pricing_lph.html).\n\nThe classic theory described there assumes that a decision maker completely trusts the statistical model that he posits to govern the joint distribution of returns on a list of available assets.\n\nBoth extensions described here put distrust of that statistical model into the mind of the decision maker.\n\nOne is a model of Black and Litterman [[BL92](https://python-advanced.quantecon.org/zreferences.html#id68)] that imputes to the decision maker distrust of historically estimated mean returns but still complete trust of estimated covariances of returns.\n\nThe second model also imputes to the decision maker doubts about his  statistical model, but now by saying that, because of that distrust,  the decision maker uses a version of  robust control theory described in this lecture [Robustness](https://python-advanced.quantecon.org/robustness.html).\n\nThe famous **Black-Litterman** (1992) [[BL92](https://python-advanced.quantecon.org/zreferences.html#id68)] portfolio choice model was motivated by the finding that with high frequency or\nmoderately high  frequency data, means are more difficult to estimate than\nvariances.\n\nA model of **robust portfolio choice** that we’ll describe below also begins\nfrom the same starting point.\n\nTo begin, we’ll take for granted that means are more difficult to\nestimate that covariances and will focus on how Black and Litterman, on\nthe one hand, an robust control theorists, on the other, would recommend\nmodifying the **mean-variance portfolio choice model** to take that into\naccount.\n\nAt the end of this lecture, we shall use some rates of convergence\nresults and some simulations to verify how means are more difficult to\nestimate than variances.\n\nAmong the ideas in play in this lecture will be\n\n- Mean-variance portfolio theory  \n- Bayesian approaches to estimating linear regressions  \n- A risk-sensitivity operator and its connection to robust control\n  theory  \n\n\nIn summary, we’ll describe two ways to  modify the classic\nmean-variance portfolio choice model in ways designed to make its\nrecommendations more plausible.\n\nBoth of the adjustments that we describe are designed to confront a\nwidely recognized embarrassment to mean-variance portfolio theory,\nnamely, that it usually implies taking very extreme long-short portfolio\npositions.\n\nThe two approaches build on a common and widespread hunch –\nthat because it is much easier statistically to estimate covariances of\nexcess returns than it is to estimate their means, it makes sense to\nadjust investors’ subjective beliefs about mean returns in order to render more plausible decisions.\n\nLet’s start with some imports:","metadata":{},"id":"98782aac"},{"cell_type":"code","source":"import numpy as np\nimport scipy as sp\nimport scipy.stats as stat\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom ipywidgets import interact, FloatSlider","metadata":{"hide-output":false},"execution_count":2,"outputs":[],"id":"f41904a1"},{"cell_type":"markdown","source":"## Mean-Variance Portfolio Choice\n\nA risk-free security earns one-period net return $ r_f $.\n\nAn $ n \\times 1 $ vector of risky securities earns\nan $ n \\times 1 $ vector $ \\vec r - r_f {\\bf 1} $ of *excess\nreturns*, where $ {\\bf 1} $ is an $ n \\times 1 $ vector of\nones.\n\nThe excess return vector is multivariate normal with mean $ \\mu $\nand covariance matrix $ \\Sigma $, which we express either as\n\n$$\n\\vec r - r_f {\\bf 1} \\sim {\\mathcal N}(\\mu, \\Sigma)\n$$\n\nor\n\n$$\n\\vec r - r_f {\\bf 1} = \\mu + C \\epsilon\n$$\n\nwhere $ \\epsilon \\sim {\\mathcal N}(0, I) $ is an $ n \\times 1 $\nrandom vector.\n\nLet $ w $ be an $ n \\times 1 $  vector of portfolio weights.\n\nA portfolio consisting $ w $ earns returns\n\n$$\nw' (\\vec r - r_f {\\bf 1}) \\sim {\\mathcal N}(w' \\mu, w' \\Sigma w ) \n$$ \nSource: https://profs.degroote.mcmaster.ca/ads/balvers/notes/2.pdf <br> <br>\n\nThe **mean-variance portfolio choice problem** is to choose $ w $ to\nmaximize\n\n\n<a id='equation-choice-problem'></a>\n$$\nU(\\mu,\\Sigma;w) = w'\\mu - \\frac{\\delta}{2} w' \\Sigma w \\tag{36.1}\n$$\n\nwhere $ \\delta > 0 $ is a risk-aversion parameter. The first-order\ncondition for maximizing [(36.1)](#equation-choice-problem) with respect to the vector $ w $ is\n\n$$\n\\mu = \\delta \\Sigma w\n$$\n\nwhich implies the following design of a risky portfolio:\n\n\n<a id='equation-risky-portfolio'></a>\n$$\nw = (\\delta \\Sigma)^{-1} \\mu \\tag{36.2}\n$$","metadata":{},"id":"f3f574ea"},{"cell_type":"markdown","source":"## Estimating  Mean and Variance\n\nThe key inputs into the portfolio choice model [(36.2)](#equation-risky-portfolio) are\n\n- estimates of the parameters $ \\mu, \\Sigma $ of the random excess\n  return vector$ (\\vec r - r_f {\\bf 1}) $  \n- the risk-aversion parameter $ \\delta $  \n\n\nA standard way of estimating $ \\mu $ is maximum-likelihood or least\nsquares; that amounts to estimating $ \\mu $ by a sample mean of\nexcess returns and estimating $ \\Sigma $ by a sample covariance\nmatrix.","metadata":{},"id":"e6e16c50"},{"cell_type":"markdown","source":"## Black-Litterman Starting Point\n\nWhen estimates of $ \\mu $ and $ \\Sigma $ from historical\nsample means and covariances have been combined with **plausible** values\nof the risk-aversion parameter $ \\delta $ to compute an\noptimal portfolio from formula [(36.2)](#equation-risky-portfolio), a typical outcome has been\n$ w $’s with **extreme long and short positions**.\n\nA common reaction to these outcomes is that they are so implausible that a portfolio\nmanager cannot recommend them to a customer.","metadata":{},"id":"5a82c944"},{"cell_type":"code","source":"np.random.seed(12)\n\nN = 10                                           # Number of assets\nT = 200                                          # Sample size\n\n# random market portfolio (sum is normalized to 1)\nw_m = np.random.rand(N)\nw_m = w_m / (w_m.sum())\n\n# True risk premia and variance of excess return (constructed\n# so that the Sharpe ratio is 1)\nμ = (np.random.randn(N) + 5)  /100      # Mean excess return (risk premium)\nS = np.random.randn(N, N)        # Random matrix for the covariance matrix = VCV Matrix randomly generated (10x10) but not symetric)\nV = S @ S.T           # Turn the random VCV matrix into a symmetric matrix (M * Traspose(M)= A (symetric matric)\n# Make sure that the Sharpe ratio is one\nΣ = V * (w_m @ μ)**2 / (w_m @ V @ w_m)\n\n# Risk aversion of market portfolio holder\nδ = 1 / np.sqrt(w_m @ Σ @ w_m)\n\n# Generate a sample of excess returns\nexcess_return = stat.multivariate_normal(μ, Σ)\nsample = excess_return.rvs(T)\n\n# Estimate μ and Σ\nμ_est = sample.mean(0).reshape(N, 1)\nΣ_est = np.cov(sample.T)\n\nw = np.linalg.solve(δ * Σ_est, μ_est)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.set_title('Mean-variance portfolio weights recommendation and the market portfolio')\nax.plot(np.arange(N)+1, w, 'o', c='k', label='$w$ (mean-variance)')\nax.plot(np.arange(N)+1, w_m, 'o', c='r', label='$w_m$ (market portfolio)')\nax.vlines(np.arange(N)+1, 0, w, lw=1)\nax.vlines(np.arange(N)+1, 0, w_m, lw=1)\nax.axhline(0, c='k')\nax.axhline(-1, c='k', ls='--')\nax.axhline(1, c='k', ls='--')\nax.set_xlabel('Assets')\nax.xaxis.set_ticks(np.arange(1, N+1, 1))\nplt.legend(numpoints=1, fontsize=11)\nplt.show()","metadata":{"hide-output":false},"execution_count":null,"outputs":[],"id":"0f60202e"},{"cell_type":"code","source":"μ = (np.random.randn(N) + 5)  /100      # Mean excess return (risk premium)\nS = np.random.randn(N, N)        # Random matrix for the covariance matrix\nV = S @ S.T           # Turn the random matrix into symmetric psd\n# Make sure that the Sharpe ratio is one\nΣ = V * (w_m @ μ)**2 / (w_m @ V @ w_m)\nprint('S :',S)\nprint('μ :',μ)\nprint('V :',V)","metadata":{},"execution_count":7,"outputs":[{"name":"stdout","text":"S : [[ 2.11393545e-03  1.22881951e+00  7.27619846e-01 -2.63955180e-01\n   1.11690546e+00 -1.32339695e+00 -2.61593353e-01 -5.45785573e-01\n   1.10438579e+00 -1.32096983e+00]\n [ 2.25277217e+00 -1.60246979e+00  1.21836289e+00 -3.38387467e-01\n  -4.47432656e-01  3.54187542e-01 -2.76643080e-01  8.96324134e-01\n   1.39223223e+00  9.95064029e-01]\n [-8.88944016e-01  3.65030974e-01  4.70742047e-01 -5.38941882e-02\n   1.06220089e+00  1.87179046e-01  7.75647336e-01  1.22075595e+00\n   1.34664061e+00 -6.23053236e-01]\n [-5.74413903e-01  1.39229694e+00 -1.57016037e+00 -1.55927696e+00\n  -1.86046162e+00  1.01344401e+00  2.46228684e-01 -1.42873504e+00\n   4.12227832e-01 -1.27271924e+00]\n [ 2.46313217e-01 -2.13003439e-01 -3.11286874e-01  7.00766700e-01\n   1.54322243e-01 -9.09329357e-01 -2.25650308e+00 -1.12403931e+00\n  -8.46006598e-01 -8.91973880e-01]\n [-1.10240156e+00 -1.18212811e-01  4.70790726e-02 -1.36618632e+00\n  -2.39601673e-02 -2.59957753e+00 -1.93726830e+00 -2.66910641e-01\n  -8.04666159e-01 -7.54268211e-01]\n [-5.17281785e-01 -7.42099895e-01 -2.87215467e-01 -1.03342848e-01\n   7.27345385e-01 -8.34836980e-01 -1.17675097e+00 -9.47500616e-02\n   1.57773521e+00  2.87578194e-01]\n [-1.10808653e+00  1.84606277e+00 -9.88955546e-01 -3.81012831e-01\n   1.23511394e+00 -3.27389772e-01  3.18928039e-01  1.37913625e-01\n  -6.01161326e-01  4.01466648e-01]\n [ 2.50920177e-01 -2.39768281e-01 -2.27749438e+00  5.28725066e-01\n  -6.83642174e-02 -1.74071374e+00 -8.50698869e-01 -9.48275177e-01\n   1.28960623e+00 -1.98627312e-01]\n [ 1.51417004e-01  1.06643551e+00  2.07530844e+00 -2.78834776e+00\n   3.81730738e-01  1.84116249e+00  4.96188245e-01 -4.64468267e-01\n   1.56313105e+00  2.31476004e-01]]\nμ : [0.04780152 0.06580483 0.04960502 0.03961001 0.05193917 0.05253177\n 0.05505696 0.05788773 0.04196777 0.04613201]\nV : [[ 8.43890439 -2.15075133  3.18316083  0.41147178  2.15079902  4.42094291\n   2.54458031  2.1069542   2.56318311  3.09080704]\n [-2.15075133 13.37584229 -0.27026043 -5.7603897  -2.55996023 -4.2585187\n   1.81113075 -7.60117588 -1.60702941  4.43850979]\n [ 3.18316083 -0.27026043  6.6047886  -1.62779534 -4.19324867 -1.92154005\n   1.59268854  1.82059824 -1.76668945  3.91061176]\n [ 0.41147178 -5.7603897  -1.62779534 15.54523496 -0.41381041  0.46763875\n  -2.19323085  1.84673535  2.56649091  4.77831795]\n [ 2.15079902 -2.55996023 -4.19324867 -0.41381041  9.41132447  7.16679562\n   2.08958934 -0.86118937  4.83633362 -6.53162895]\n [ 4.42094291 -4.2585187  -1.92154005  0.46763875  7.16679562 14.89698032\n   3.75694877  1.82504587  4.46218041 -3.45094537]\n [ 2.54458031  1.81113075  1.59268854 -2.19323085  2.08958934  3.75694877\n   6.10311929 -0.52306676  5.11956116 -0.44415952]\n [ 2.1069542  -7.60117588  1.82059824  1.84673535 -0.86118937  1.82504587\n  -0.52306676  8.03500355  0.55857996 -0.07293679]\n [ 2.56318311 -1.60702941 -1.76668945  2.56649091  4.83633362  4.46218041\n   5.11956116  0.55857996 11.94719001 -7.66132703]\n [ 3.09080704  4.43850979  3.91061176  4.77831795 -6.53162895 -3.45094537\n  -0.44415952 -0.07293679 -7.66132703 19.73649118]]\n","output_type":"stream"}],"id":"c7148728-0cf4-4f36-9c6c-2dbedbcde941"},{"cell_type":"code","source":"a=np.random.randn(N)\nprint(a)\nprint(' Plus 5:',a+5)\n","metadata":{},"execution_count":10,"outputs":[{"name":"stdout","text":"[-0.88851132 -1.3342008   1.90629713 -0.70390117 -1.28906287 -2.03105207\n -0.266293    0.72705349  1.54358949 -2.02588698]\n Plus 5: [4.11148868 3.6657992  6.90629713 4.29609883 3.71093713 2.96894793\n 4.733707   5.72705349 6.54358949 2.97411302]\n","output_type":"stream"}],"id":"e361c7ab-33a2-4888-9c43-0ded70b7d2fa"},{"cell_type":"markdown","source":"Black and Litterman’s responded to this situation in the following way:\n\n- They continue to accept [(36.2)](#equation-risky-portfolio) as a good model for choosing an optimal\n  portfolio $ w $.  \n- They want to continue to allow the customer to express his or her\n  risk tolerance by setting $ \\delta $.  \n- Leaving $ \\Sigma $ at its maximum-likelihood value, they push\n  $ \\mu $ away from its maximum-likelihood value in a way designed to make\n  portfolio choices that are more plausible in terms of conforming to\n  what most people actually do.  \n\n\nIn particular, given $ \\Sigma $ and a plausible value\nof $ \\delta $, Black and Litterman reverse engineered a vector\n$ \\mu_{BL} $ of mean excess returns that makes the $ w $\nimplied by formula [(36.2)](#equation-risky-portfolio) equal the **actual** market portfolio\n$ w_m $, so that\n\n$$\nw_m = (\\delta \\Sigma)^{-1} \\mu_{BL}\n$$","metadata":{},"id":"8c29c383"},{"cell_type":"markdown","source":"## Details\n\nLet’s define\n\n$$\nw_m' \\mu \\equiv ( r_m - r_f)\n$$\n\nas the (scalar) excess return on the market portfolio $ w_m $.\n\nDefine\n\n$$\n\\sigma^2 = w_m' \\Sigma w_m\n$$\n\nas the variance of the excess return on the market portfolio\n$ w_m $.\n\nDefine\n\n$$\n{\\bf SR}_m = \\frac{ r_m - r_f}{\\sigma}\n$$\n\nas the **Sharpe-ratio** on the market portfolio $ w_m $.\n\nLet $ \\delta_m $ be the value of the risk aversion parameter that\ninduces an investor to hold the market portfolio in light of the optimal\nportfolio choice rule [(36.2)](#equation-risky-portfolio).\n\nEvidently, portfolio rule [(36.2)](#equation-risky-portfolio) then implies that\n$ r_m - r_f = \\delta_m \\sigma^2 $ or\n\n$$\n\\delta_m = \\frac{r_m - r_f}{\\sigma^2}\n$$\n\nor\n\n$$\n\\delta_m = \\frac{{\\bf SR}_m}{\\sigma}\n$$\n\nFollowing the Black-Litterman philosophy, our first step will be to back\na value of $ \\delta_m $ from\n\n- an estimate of the Sharpe-ratio, and  \n- our maximum likelihood estimate of $ \\sigma $ drawn from our\n  estimates or $ w_m $ and $ \\Sigma $  \n\n\nThe second key Black-Litterman step is then to use this value of\n$ \\delta $ together with the maximum likelihood estimate of\n$ \\Sigma $ to deduce a $ \\mu_{\\bf BL} $ that verifies\nportfolio rule [(36.2)](#equation-risky-portfolio) at the market portfolio $ w = w_m $\n\n$$\n\\mu_m = \\delta_m \\Sigma w_m\n$$\n\nThe starting point of the Black-Litterman portfolio choice model is thus\na pair $ (\\delta_m, \\mu_m) $ that tells the customer to hold the\nmarket portfolio.","metadata":{},"id":"1fac6d56"},{"cell_type":"code","source":"# Observed mean excess market return\nr_m = w_m @ μ_est\n\n# Estimated variance of the market portfolio\nσ_m = w_m @ Σ_est @ w_m\n\n# Sharpe-ratio\nsr_m = r_m / np.sqrt(σ_m)\n\n# Risk aversion of market portfolio holder\nd_m = r_m / σ_m\n\n# Derive \"view\" which would induce the market portfolio\nμ_m = (d_m * Σ_est @ w_m).reshape(N, 1)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.set_title(r'Difference between $\\hat{\\mu}$ (estimate) and $\\mu_{BL}$ (market implied)')\nax.plot(np.arange(N)+1, μ_est, 'o', c='k', label='$\\hat{\\mu}$')\nax.plot(np.arange(N)+1, μ_m, 'o', c='r', label='$\\mu_{BL}$')\nax.vlines(np.arange(N) + 1, μ_m, μ_est, lw=1)\nax.axhline(0, c='k', ls='--')\nax.set_xlabel('Assets')\nax.xaxis.set_ticks(np.arange(1, N+1, 1))\nplt.legend(numpoints=1)\nplt.show()","metadata":{"hide-output":false},"execution_count":null,"outputs":[],"id":"91697c59"},{"cell_type":"markdown","source":"## Adding Views\n\nBlack and Litterman start with a baseline customer who asserts that he\nor she shares the **market’s views**, which means that he or she\nbelieves that excess returns are governed by\n\n\n<a id='equation-excess-returns'></a>\n$$\n\\vec r - r_f {\\bf 1} \\sim {\\mathcal N}( \\mu_{BL}, \\Sigma) \\tag{36.3}\n$$\n\nBlack and Litterman would advise that customer to hold the market\nportfolio of risky securities.\n\nBlack and Litterman then imagine a consumer who would like to express a\nview that differs from the market’s.\n\nThe consumer wants appropriately to\nmix his view with the market’s before using [(36.2)](#equation-risky-portfolio) to choose a portfolio.\n\nSuppose that the customer’s view is expressed by a hunch that rather\nthan [(36.3)](#equation-excess-returns), excess returns are governed by\n\n$$\n\\vec r - r_f {\\bf 1} \\sim {\\mathcal N}( \\hat \\mu, \\tau \\Sigma)\n$$\n\nwhere $ \\tau > 0 $ is a scalar parameter that determines how the\ndecision maker wants to mix his view $ \\hat \\mu $ with the market’s\nview $ \\mu_{\\bf BL} $.\n\nBlack and Litterman would then use a formula like the following one to\nmix the views $ \\hat \\mu $ and $ \\mu_{\\bf BL} $\n\n\n<a id='equation-mix-views'></a>\n$$\n\\tilde \\mu = (\\Sigma^{-1} + (\\tau \\Sigma)^{-1})^{-1} (\\Sigma^{-1} \\mu_{BL}  + (\\tau \\Sigma)^{-1} \\hat \\mu) \\tag{36.4}\n$$\n\nBlack and Litterman would then advise the customer to hold the portfolio\nassociated with these views implied by rule [(36.2)](#equation-risky-portfolio):\n\n$$\n\\tilde w = (\\delta \\Sigma)^{-1} \\tilde \\mu\n$$\n\nThis portfolio $ \\tilde w $ will deviate from the\nportfolio $ w_{BL} $ in amounts that depend on the mixing parameter\n$ \\tau $.\n\nIf $ \\hat \\mu $ is the maximum likelihood estimator\nand $ \\tau $ is chosen heavily to weight this view, then the\ncustomer’s portfolio will involve big short-long positions.","metadata":{},"id":"27e2eac7"},{"cell_type":"code","source":"def black_litterman(λ, μ1, μ2, Σ1, Σ2):\n    \"\"\"\n    This function calculates the Black-Litterman mixture\n    mean excess return and covariance matrix\n    \"\"\"\n    Σ1_inv = np.linalg.inv(Σ1)\n    Σ2_inv = np.linalg.inv(Σ2)\n\n    μ_tilde = np.linalg.solve(Σ1_inv + λ * Σ2_inv,\n                              Σ1_inv @ μ1 + λ * Σ2_inv @ μ2)\n    return μ_tilde\n\nτ = 1\nμ_tilde = black_litterman(1, μ_m, μ_est, Σ_est, τ * Σ_est)\n\n# The Black-Litterman recommendation for the portfolio weights\nw_tilde = np.linalg.solve(δ * Σ_est, μ_tilde)\n\nτ_slider = FloatSlider(min=0.05, max=10, step=0.5, value=τ)\n\n@interact(τ=τ_slider)\ndef BL_plot(τ):\n    μ_tilde = black_litterman(1, μ_m, μ_est, Σ_est, τ * Σ_est)\n    w_tilde = np.linalg.solve(δ * Σ_est, μ_tilde)\n\n    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n    ax[0].plot(np.arange(N)+1, μ_est, 'o', c='k',\n               label=r'$\\hat{\\mu}$ (subj view)')\n    ax[0].plot(np.arange(N)+1, μ_m, 'o', c='r',\n               label=r'$\\mu_{BL}$ (market)')\n    ax[0].plot(np.arange(N)+1, μ_tilde, 'o', c='y',\n               label=r'$\\tilde{\\mu}$ (mixture)')\n    ax[0].vlines(np.arange(N)+1, μ_m, μ_est, lw=1)\n    ax[0].axhline(0, c='k', ls='--')\n    ax[0].set(xlim=(0, N+1), xlabel='Assets',\n              title=r'Relationship between $\\hat{\\mu}$, $\\mu_{BL}$, and  $ \\tilde{\\mu}$')\n    ax[0].xaxis.set_ticks(np.arange(1, N+1, 1))\n    ax[0].legend(numpoints=1)\n\n    ax[1].set_title('Black-Litterman portfolio weight recommendation')\n    ax[1].plot(np.arange(N)+1, w, 'o', c='k', label=r'$w$ (mean-variance)')\n    ax[1].plot(np.arange(N)+1, w_m, 'o', c='r', label=r'$w_{m}$ (market, BL)')\n    ax[1].plot(np.arange(N)+1, w_tilde, 'o', c='y',\n               label=r'$\\tilde{w}$ (mixture)')\n    ax[1].vlines(np.arange(N)+1, 0, w, lw=1)\n    ax[1].vlines(np.arange(N)+1, 0, w_m, lw=1)\n    ax[1].axhline(0, c='k')\n    ax[1].axhline(-1, c='k', ls='--')\n    ax[1].axhline(1, c='k', ls='--')\n    ax[1].set(xlim=(0, N+1), xlabel='Assets',\n              title='Black-Litterman portfolio weight recommendation')\n    ax[1].xaxis.set_ticks(np.arange(1, N+1, 1))\n    ax[1].legend(numpoints=1)\n    plt.show()","metadata":{"hide-output":false},"execution_count":null,"outputs":[],"id":"a7adf96c"},{"cell_type":"markdown","source":"## Bayesian Interpretation\n\nConsider the following Bayesian interpretation of the Black-Litterman\nrecommendation.\n\nThe prior belief over the mean excess returns is consistent with the\nmarket portfolio and is given by\n\n$$\n\\mu \\sim \\mathcal{N}(\\mu_{BL}, \\Sigma)\n$$\n\nGiven a particular realization of the mean excess returns\n$ \\mu $ one observes the average excess returns $ \\hat \\mu $\non the market according to the distribution\n\n$$\n\\hat \\mu \\mid \\mu, \\Sigma \\sim \\mathcal{N}(\\mu, \\tau\\Sigma)\n$$\n\nwhere $ \\tau $ is typically small capturing the idea that the\nvariation in the mean is smaller than the variation of the individual\nrandom variable.\n\nGiven the realized excess returns one should then update the prior over\nthe mean excess returns according to Bayes rule.\n\nThe corresponding\nposterior over mean excess returns is normally distributed with mean\n\n$$\n(\\Sigma^{-1} + (\\tau \\Sigma)^{-1})^{-1} (\\Sigma^{-1}\\mu_{BL}   + (\\tau \\Sigma)^{-1} \\hat \\mu)\n$$\n\nThe covariance matrix is\n\n$$\n(\\Sigma^{-1} + (\\tau \\Sigma)^{-1})^{-1}\n$$\n\nHence, the Black-Litterman recommendation is consistent with the Bayes\nupdate of the prior over the mean excess returns in light of the\nrealized average excess returns on the market.","metadata":{},"id":"5341abb9"},{"cell_type":"markdown","source":"## Curve Decolletage\n\nConsider two independent “competing” views on the excess market returns\n\n$$\n\\vec r_e  \\sim {\\mathcal N}( \\mu_{BL}, \\Sigma)\n$$\n\nand\n\n$$\n\\vec r_e \\sim {\\mathcal N}( \\hat{\\mu}, \\tau\\Sigma)\n$$\n\nA special feature of the multivariate normal random variable\n$ Z $ is that its density function depends only on the (Euclidiean)\nlength of its realization $ z $.\n\nFormally, let the\n$ k $-dimensional random vector be\n\n$$\nZ\\sim \\mathcal{N}(\\mu, \\Sigma)\n$$\n\nthen\n\n$$\n\\bar{Z} \\equiv \\Sigma(Z-\\mu)\\sim \\mathcal{N}(\\mathbf{0}, I)\n$$\n\nand so the points where the density takes the same value can be\ndescribed by the ellipse\n\n\n<a id='equation-ellipse'></a>\n$$\n\\bar z \\cdot \\bar z =  (z - \\mu)'\\Sigma^{-1}(z - \\mu) = \\bar d \\tag{36.5}\n$$\n\nwhere $ \\bar d\\in\\mathbb{R}_+ $ denotes the (transformation) of a\nparticular density value.\n\nThe curves defined by equation [(36.5)](#equation-ellipse) can be\nlabeled as iso-likelihood ellipses\n\n> **Remark:** More generally there is a class of density functions\nthat possesses this feature, i.e.\n\n$$\n\\exists g: \\mathbb{R}_+ \\mapsto \\mathbb{R}_+ \\ \\ \\text{ and } \\ \\ c \\geq 0,\n  \\ \\ \\text{s.t.  the density } \\ \\ f \\ \\ \\text{of} \\ \\ Z  \\ \\\n  \\text{ has the form } \\quad f(z) = c g(z\\cdot z)\n$$\n\nThis property is called **spherical symmetry** (see p 81. in Leamer\n(1978) [[Lea78](https://python-advanced.quantecon.org/zreferences.html#id67)]).\n\n\nIn our specific example, we can use the pair\n$ (\\bar d_1, \\bar d_2) $ as being two “likelihood” values for which\nthe corresponding iso-likelihood ellipses in the excess return space are\ngiven by\n\n$$\n\\begin{aligned}\n(\\vec r_e - \\mu_{BL})'\\Sigma^{-1}(\\vec r_e - \\mu_{BL}) &= \\bar d_1 \\\\\n(\\vec r_e - \\hat \\mu)'\\left(\\tau \\Sigma\\right)^{-1}(\\vec r_e - \\hat \\mu) &= \\bar d_2\n\\end{aligned}\n$$\n\nNotice that for particular $ \\bar d_1 $ and $ \\bar d_2 $ values\nthe two ellipses have a tangency point.\n\nThese tangency points, indexed\nby the pairs $ (\\bar d_1, \\bar d_2) $, characterize points\n$ \\vec r_e $ from which there exists no deviation where one can\nincrease the likelihood of one view without decreasing the likelihood of\nthe other view.\n\nThe pairs $ (\\bar d_1, \\bar d_2) $ for which there\nis such a point outlines a curve in the excess return space. This curve\nis reminiscent of the Pareto curve in an Edgeworth-box setting.\n\nDickey (1975) [[Dic75](https://python-advanced.quantecon.org/zreferences.html#id60)] calls it a *curve decolletage*.\n\nLeamer (1978) [[Lea78](https://python-advanced.quantecon.org/zreferences.html#id67)] calls it an *information contract curve* and\ndescribes it by the following program: maximize the likelihood of one\nview, say the Black-Litterman recommendation while keeping the\nlikelihood of the other view at least at a prespecified constant\n$ \\bar d_2 $\n\n$$\n\\begin{aligned}\n \\bar d_1(\\bar d_2) &\\equiv \\max_{\\vec r_e} \\ \\ (\\vec r_e - \\mu_{BL})'\\Sigma^{-1}(\\vec r_e - \\mu_{BL}) \\\\\n\\text{subject to }  \\quad  &(\\vec r_e - \\hat\\mu)'(\\tau\\Sigma)^{-1}(\\vec r_e - \\hat \\mu) \\geq \\bar d_2\n\\end{aligned}\n$$\n\nDenoting the multiplier on the constraint by $ \\lambda $, the\nfirst-order condition is\n\n$$\n2(\\vec r_e - \\mu_{BL} )'\\Sigma^{-1} + \\lambda 2(\\vec r_e - \\hat\\mu)'(\\tau\\Sigma)^{-1} = \\mathbf{0}\n$$\n\nwhich defines the *information contract curve* between\n$ \\mu_{BL} $ and $ \\hat \\mu $\n\n\n<a id='equation-info-curve'></a>\n$$\n\\vec r_e = (\\Sigma^{-1} + \\lambda (\\tau \\Sigma)^{-1})^{-1} (\\Sigma^{-1} \\mu_{BL} + \\lambda (\\tau \\Sigma)^{-1}\\hat \\mu ) \\tag{36.6}\n$$\n\nNote that if $ \\lambda = 1 $, [(36.6)](#equation-info-curve) is equivalent with [(36.4)](#equation-mix-views) and it\nidentifies one point on the information contract curve.\n\nFurthermore, because $ \\lambda $ is a function of the minimum likelihood\n$ \\bar d_2 $ on the RHS of the constraint, by varying\n$ \\bar d_2 $ (or $ \\lambda $ ), we can trace out the whole curve\nas the figure below illustrates.","metadata":{},"id":"8eca4f20"},{"cell_type":"code","source":"np.random.seed(1987102)\n\nN = 2                                           # Number of assets\nT = 200                                         # Sample size\nτ = 0.8\n\n# Random market portfolio (sum is normalized to 1)\nw_m = np.random.rand(N)\nw_m = w_m / (w_m.sum())\n\nμ = (np.random.randn(N) + 5) / 100\nS = np.random.randn(N, N)\nV = S @ S.T\nΣ = V * (w_m @ μ)**2 / (w_m @ V @ w_m)\n\nexcess_return = stat.multivariate_normal(μ, Σ)\nsample = excess_return.rvs(T)\n\nμ_est = sample.mean(0).reshape(N, 1)\nΣ_est = np.cov(sample.T)\n\nσ_m = w_m @ Σ_est @ w_m\nd_m = (w_m @ μ_est) / σ_m\nμ_m = (d_m * Σ_est @ w_m).reshape(N, 1)\n\nN_r1, N_r2 = 100, 100\nr1 = np.linspace(-0.04, .1, N_r1)\nr2 = np.linspace(-0.02, .15, N_r2)\n\nλ_grid = np.linspace(.001, 20, 100)\ncurve = np.asarray([black_litterman(λ, μ_m, μ_est, Σ_est,\n                                    τ * Σ_est).flatten() for λ in λ_grid])\n\nλ_slider = FloatSlider(min=.1, max=7, step=.5, value=1)\n\n@interact(λ=λ_slider)\ndef decolletage(λ):\n    dist_r_BL = stat.multivariate_normal(μ_m.squeeze(), Σ_est)\n    dist_r_hat = stat.multivariate_normal(μ_est.squeeze(), τ * Σ_est)\n\n    X, Y = np.meshgrid(r1, r2)\n    Z_BL = np.zeros((N_r1, N_r2))\n    Z_hat = np.zeros((N_r1, N_r2))\n\n    for i in range(N_r1):\n        for j in range(N_r2):\n            Z_BL[i, j] = dist_r_BL.pdf(np.hstack([X[i, j], Y[i, j]]))\n            Z_hat[i, j] = dist_r_hat.pdf(np.hstack([X[i, j], Y[i, j]]))\n\n    μ_tilde = black_litterman(λ, μ_m, μ_est, Σ_est, τ * Σ_est).flatten()\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.contourf(X, Y, Z_hat, cmap='viridis', alpha =.4)\n    ax.contourf(X, Y, Z_BL, cmap='viridis', alpha =.4)\n    ax.contour(X, Y, Z_BL, [dist_r_BL.pdf(μ_tilde)], cmap='viridis', alpha=.9)\n    ax.contour(X, Y, Z_hat, [dist_r_hat.pdf(μ_tilde)], cmap='viridis', alpha=.9)\n    ax.scatter(μ_est[0], μ_est[1])\n    ax.scatter(μ_m[0], μ_m[1])\n    ax.scatter(μ_tilde[0], μ_tilde[1], c='k', s=20*3)\n\n    ax.plot(curve[:, 0], curve[:, 1], c='k')\n    ax.axhline(0, c='k', alpha=.8)\n    ax.axvline(0, c='k', alpha=.8)\n    ax.set_xlabel(r'Excess return on the first asset, $r_{e, 1}$')\n    ax.set_ylabel(r'Excess return on the second asset, $r_{e, 2}$')\n    ax.text(μ_est[0] + 0.003, μ_est[1], r'$\\hat{\\mu}$')\n    ax.text(μ_m[0] + 0.003, μ_m[1] + 0.005, r'$\\mu_{BL}$')\n    plt.show()","metadata":{"hide-output":false},"execution_count":null,"outputs":[],"id":"550112b9"},{"cell_type":"markdown","source":"Note that the line that connects the two points\n$ \\hat \\mu $ and $ \\mu_{BL} $ is linear, which comes from the\nfact that the covariance matrices of the two competing distributions\n(views) are proportional to each other.\n\nTo illustrate the fact that this is not necessarily the case, consider\nanother example using the same parameter values, except that the “second\nview” constituting the constraint has covariance matrix\n$ \\tau I $ instead of $ \\tau \\Sigma $.\n\nThis leads to the\nfollowing figure, on which the curve connecting $ \\hat \\mu $\nand $ \\mu_{BL} $ are bending","metadata":{},"id":"04a2de44"},{"cell_type":"code","source":"λ_grid = np.linspace(.001, 20000, 1000)\ncurve = np.asarray([black_litterman(λ, μ_m, μ_est, Σ_est,\n                                    τ * np.eye(N)).flatten() for λ in λ_grid])\n\nλ_slider = FloatSlider(min=5, max=1500, step=100, value=200)\n\n@interact(λ=λ_slider)\ndef decolletage(λ):\n    dist_r_BL = stat.multivariate_normal(μ_m.squeeze(), Σ_est)\n    dist_r_hat = stat.multivariate_normal(μ_est.squeeze(), τ * np.eye(N))\n\n    X, Y = np.meshgrid(r1, r2)\n    Z_BL = np.zeros((N_r1, N_r2))\n    Z_hat = np.zeros((N_r1, N_r2))\n\n    for i in range(N_r1):\n        for j in range(N_r2):\n            Z_BL[i, j] = dist_r_BL.pdf(np.hstack([X[i, j], Y[i, j]]))\n            Z_hat[i, j] = dist_r_hat.pdf(np.hstack([X[i, j], Y[i, j]]))\n\n    μ_tilde = black_litterman(λ, μ_m, μ_est, Σ_est, τ * np.eye(N)).flatten()\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.contourf(X, Y, Z_hat, cmap='viridis', alpha=.4)\n    ax.contourf(X, Y, Z_BL, cmap='viridis', alpha=.4)\n    ax.contour(X, Y, Z_BL, [dist_r_BL.pdf(μ_tilde)], cmap='viridis', alpha=.9)\n    ax.contour(X, Y, Z_hat, [dist_r_hat.pdf(μ_tilde)], cmap='viridis', alpha=.9)\n    ax.scatter(μ_est[0], μ_est[1])\n    ax.scatter(μ_m[0], μ_m[1])\n\n    ax.scatter(μ_tilde[0], μ_tilde[1], c='k', s=20*3)\n\n    ax.plot(curve[:, 0], curve[:, 1], c='k')\n    ax.axhline(0, c='k', alpha=.8)\n    ax.axvline(0, c='k', alpha=.8)\n    ax.set_xlabel(r'Excess return on the first asset, $r_{e, 1}$')\n    ax.set_ylabel(r'Excess return on the second asset, $r_{e, 2}$')\n    ax.text(μ_est[0] + 0.003, μ_est[1], r'$\\hat{\\mu}$')\n    ax.text(μ_m[0] + 0.003, μ_m[1] + 0.005, r'$\\mu_{BL}$')\n    plt.show()","metadata":{"hide-output":false},"execution_count":null,"outputs":[],"id":"2bcf8b79"},{"cell_type":"markdown","source":"## Black-Litterman Recommendation as Regularization\n\nFirst, consider the OLS regression\n\n$$\n\\min_{\\beta} \\Vert X\\beta - y \\Vert^2\n$$\n\nwhich yields the solution\n\n$$\n\\hat{\\beta}_{OLS} = (X'X)^{-1}X'y\n$$\n\nA common performance measure of estimators is the *mean squared error\n(MSE)*.\n\nAn estimator is “good” if its MSE is relatively small. Suppose\nthat $ \\beta_0 $ is the “true” value of the coefficient, then the MSE\nof the OLS estimator is\n\n$$\n\\text{mse}(\\hat \\beta_{OLS}, \\beta_0) := \\mathbb E \\Vert \\hat \\beta_{OLS} - \\beta_0\\Vert^2 =\n\\underbrace{\\mathbb E \\Vert \\hat \\beta_{OLS} - \\mathbb E\n\\beta_{OLS}\\Vert^2}_{\\text{variance}} +\n\\underbrace{\\Vert \\mathbb E \\hat\\beta_{OLS} - \\beta_0\\Vert^2}_{\\text{bias}}\n$$\n\nFrom this decomposition, one can see that in order for the MSE to be\nsmall, both the bias and the variance terms must be small.\n\nFor example,\nconsider the case when $ X $ is a $ T $-vector of ones (where\n$ T $ is the sample size), so $ \\hat\\beta_{OLS} $ is simply the\nsample average, while $ \\beta_0\\in \\mathbb{R} $ is defined by the\ntrue mean of $ y $.\n\nIn this example the MSE is\n\n$$\n\\text{mse}(\\hat \\beta_{OLS}, \\beta_0) = \\underbrace{\\frac{1}{T^2}\n\\mathbb E \\left(\\sum_{t=1}^{T} (y_{t}- \\beta_0)\\right)^2 }_{\\text{variance}} +\n\\underbrace{0}_{\\text{bias}}\n$$\n\nHowever, because there is a trade-off between the estimator’s bias and\nvariance, there are cases when by permitting a small bias we can\nsubstantially reduce the variance so overall the MSE gets smaller.\n\nA typical scenario when this proves to be useful is when the number of\ncoefficients to be estimated is large relative to the sample size.\n\nIn these cases, one approach to handle the bias-variance trade-off is the\nso called *Tikhonov regularization*.\n\nA general form with regularization matrix $ \\Gamma $ can be written as\n\n$$\n\\min_{\\beta} \\Big\\{ \\Vert X\\beta - y \\Vert^2 + \\Vert \\Gamma (\\beta - \\tilde \\beta) \\Vert^2 \\Big\\}\n$$\n\nwhich yields the solution\n\n$$\n\\hat{\\beta}_{Reg} = (X'X + \\Gamma'\\Gamma)^{-1}(X'y + \\Gamma'\\Gamma\\tilde \\beta)\n$$\n\nSubstituting the value of $ \\hat{\\beta}_{OLS} $ yields\n\n$$\n\\hat{\\beta}_{Reg} = (X'X + \\Gamma'\\Gamma)^{-1}(X'X\\hat{\\beta}_{OLS} + \\Gamma'\\Gamma\\tilde \\beta)\n$$\n\nOften, the regularization matrix takes the form\n$ \\Gamma = \\lambda I $ with $ \\lambda>0 $\nand $ \\tilde \\beta = \\mathbf{0} $.\n\nThen the Tikhonov regularization is equivalent to what is called *ridge regression* in statistics.\n\nTo illustrate how this estimator addresses the bias-variance trade-off,\nwe compute the MSE of the ridge estimator\n\n$$\n\\text{mse}(\\hat \\beta_{\\text{ridge}}, \\beta_0) = \\underbrace{\\frac{1}{(T+\\lambda)^2}\n\\mathbb E \\left(\\sum_{t=1}^{T} (y_{t}- \\beta_0)\\right)^2 }_{\\text{variance}} +\n\\underbrace{\\left(\\frac{\\lambda}{T+\\lambda}\\right)^2 \\beta_0^2}_{\\text{bias}}\n$$\n\nThe ridge regression shrinks the coefficients of the estimated vector\ntowards zero relative to the OLS estimates thus reducing the variance\nterm at the cost of introducing a “small” bias.\n\nHowever, there is nothing special about the zero vector.\n\nWhen $ \\tilde \\beta \\neq \\mathbf{0} $ shrinkage occurs in the direction\nof $ \\tilde \\beta $.\n\nNow, we can give a regularization interpretation of the Black-Litterman\nportfolio recommendation.\n\nTo this end, first  simplify the equation [(36.4)](#equation-mix-views) that characterizes the Black-Litterman recommendation\n\n$$\n\\begin{aligned}\n\\tilde \\mu &= (\\Sigma^{-1} + (\\tau \\Sigma)^{-1})^{-1} (\\Sigma^{-1}\\mu_{BL}  + (\\tau \\Sigma)^{-1}\\hat \\mu) \\\\\n&= (1 + \\tau^{-1})^{-1}\\Sigma \\Sigma^{-1} (\\mu_{BL}  + \\tau ^{-1}\\hat \\mu) \\\\\n&= (1 + \\tau^{-1})^{-1} ( \\mu_{BL}  + \\tau ^{-1}\\hat \\mu)\n\\end{aligned}\n$$\n\nIn our case, $ \\hat \\mu $ is the estimated mean excess returns of\nsecurities. This could be written as a vector autoregression where\n\n- $ y $ is the stacked vector of observed excess returns of size\n  $ (N T\\times 1) $ – $ N $ securities and $ T $\n  observations.  \n- $ X = \\sqrt{T^{-1}}(I_{N} \\otimes \\iota_T) $ where $ I_N $\n  is the identity matrix and $ \\iota_T $ is a column vector of\n  ones.  \n\n\nCorrespondingly, the OLS regression of $ y $ on $ X $ would\nyield the mean excess returns as coefficients.\n\nWith $ \\Gamma = \\sqrt{\\tau T^{-1}}(I_{N} \\otimes \\iota_T) $ we can\nwrite the regularized version of the mean excess return estimation\n\n$$\n\\begin{aligned}\n\\hat{\\beta}_{Reg} &= (X'X + \\Gamma'\\Gamma)^{-1}(X'X\\hat{\\beta}_{OLS} + \\Gamma'\\Gamma\\tilde \\beta) \\\\\n&= (1 + \\tau)^{-1}X'X (X'X)^{-1} (\\hat \\beta_{OLS}  + \\tau \\tilde \\beta) \\\\\n&= (1 + \\tau)^{-1} (\\hat \\beta_{OLS}  + \\tau \\tilde \\beta) \\\\\n&= (1 + \\tau^{-1})^{-1} ( \\tau^{-1}\\hat \\beta_{OLS}  +  \\tilde \\beta)\n\\end{aligned}\n$$\n\nGiven that\n$ \\hat \\beta_{OLS} = \\hat \\mu $ and $ \\tilde \\beta = \\mu_{BL} $\nin the Black-Litterman model, we have the following interpretation of the\nmodel’s recommendation.\n\nThe estimated (personal) view of the mean excess returns,\n$ \\hat{\\mu} $ that would lead to extreme short-long positions are\n“shrunk” towards the conservative market view, $ \\mu_{BL} $, that\nleads to the more conservative market portfolio.\n\nSo the Black-Litterman procedure results in a recommendation that is a\ncompromise between the conservative market portfolio and the more\nextreme portfolio that is implied by estimated “personal” views.","metadata":{},"id":"a52221cf"},{"cell_type":"markdown","source":"## A Robust Control Operator\n\nThe Black-Litterman approach is partly inspired by the econometric\ninsight that it is easier to estimate covariances of excess returns than\nthe means.\n\nThat is what gave Black and Litterman license to adjust\ninvestors’ perception of mean excess returns while not tampering with\nthe covariance matrix of excess returns.\n\nThe robust control theory is another approach that also hinges on\nadjusting mean excess returns but not covariances.\n\nAssociated with a robust control problem is what Hansen and Sargent [[HS01](https://python-advanced.quantecon.org/zreferences.html#id59)], [[HS08a](https://python-advanced.quantecon.org/zreferences.html#id157)] call\na $ {\\sf T} $ operator.\n\nLet’s define the $ {\\sf T} $ operator as it applies to the problem\nat hand.\n\nLet $ x $ be an $ n \\times 1 $ Gaussian random vector with mean\nvector $ \\mu $ and covariance matrix $ \\Sigma = C C' $. This\nmeans that $ x $ can be represented as\n\n$$\nx = \\mu + C \\epsilon\n$$\n\nwhere $ \\epsilon \\sim {\\mathcal N}(0,I) $.\n\nLet $ \\phi(\\epsilon) $ denote the associated standardized Gaussian\ndensity.\n\nLet $ m(\\epsilon,\\mu) $ be a **likelihood ratio**, meaning that it\nsatisfies\n\n- $ m(\\epsilon, \\mu) > 0 $  \n- $ \\int m(\\epsilon,\\mu) \\phi(\\epsilon) d \\epsilon =1 $  \n\n\nThat is, $ m(\\epsilon, \\mu) $ is a non-negative random variable with\nmean 1.\n\nMultiplying $ \\phi(\\epsilon) $\nby the likelihood ratio $ m(\\epsilon, \\mu) $ produces a distorted distribution for\n$ \\epsilon $, namely\n\n$$\n\\tilde \\phi(\\epsilon) = m(\\epsilon,\\mu) \\phi(\\epsilon)\n$$\n\nThe next concept that we need is the **entropy** of the distorted\ndistribution $ \\tilde \\phi $ with respect to $ \\phi $.\n\n**Entropy** is defined as\n\n$$\n{\\rm ent} = \\int \\log m(\\epsilon,\\mu) m(\\epsilon,\\mu) \\phi(\\epsilon) d \\epsilon\n$$\n\nor\n\n$$\n{\\rm ent} = \\int \\log m(\\epsilon,\\mu) \\tilde \\phi(\\epsilon) d \\epsilon\n$$\n\nThat is, relative entropy is the expected value of the likelihood ratio\n$ m $ where the expectation is taken with respect to the twisted\ndensity $ \\tilde \\phi $.\n\nRelative entropy is non-negative. It is a measure of the discrepancy\nbetween two probability distributions.\n\nAs such, it plays an important\nrole in governing the behavior of statistical tests designed to\ndiscriminate one probability distribution from another.\n\nWe are ready to define the $ {\\sf T} $ operator.\n\nLet $ V(x) $ be a value function.\n\nDefine\n\n$$\n\\begin{aligned} {\\sf T}\\left(V(x)\\right) & = \\min_{m(\\epsilon,\\mu)} \\int m(\\epsilon,\\mu)[V(\\mu + C \\epsilon) + \\theta \\log m(\\epsilon,\\mu) ] \\phi(\\epsilon) d \\epsilon \\cr\n                        & = - \\log \\theta \\int \\exp \\left( \\frac{- V(\\mu + C \\epsilon)}{\\theta} \\right) \\phi(\\epsilon) d \\epsilon \\end{aligned}\n$$\n\nThis asserts that $ {\\sf T} $ is an indirect utility function for a\nminimization problem in which an **adversary** chooses a distorted\nprobability distribution $ \\tilde \\phi $ to lower expected utility,\nsubject to a penalty term that gets bigger the larger is relative\nentropy.\n\nHere the penalty parameter\n\n$$\n\\theta \\in [\\underline \\theta, +\\infty]\n$$\n\nis a robustness parameter when it is $ +\\infty $, there is no scope for the minimizing agent to distort the distribution,\nso no robustness to alternative distributions is acquired.\n\nAs $ \\theta $ is lowered, more robustness is achieved.\n\n**Note:** The $ {\\sf T} $ operator is sometimes called a\n*risk-sensitivity* operator.\n\nWe shall apply $ {\\sf T} $to the special case of a linear value\nfunction $ w'(\\vec r - r_f 1) $\nwhere $ \\vec r - r_f 1 \\sim {\\mathcal N}(\\mu,\\Sigma) $ or\n$ \\vec r - r_f {\\bf 1} = \\mu + C \\epsilon $ and\n$ \\epsilon \\sim {\\mathcal N}(0,I) $.\n\nThe associated worst-case distribution of $ \\epsilon $ is Gaussian\nwith mean $ v =-\\theta^{-1} C' w $ and covariance matrix $ I $\n\n(When the value function is affine, the worst-case distribution distorts\nthe mean vector of $ \\epsilon $ but not the covariance matrix\nof $ \\epsilon $).\n\nFor utility function argument $ w'(\\vec r - r_f 1) $\n\n$$\n{\\sf T} ( \\vec r - r_f {\\bf 1}) = w' \\mu + \\zeta - \\frac{1}{2 \\theta} w' \\Sigma w\n$$\n\nand entropy is\n\n$$\n\\frac{v'v}{2} = \\frac{1}{2\\theta^2}  w' C C' w\n$$","metadata":{},"id":"f267f492"},{"cell_type":"markdown","source":"## A Robust Mean-Variance Portfolio Model\n\nAccording to criterion [(36.1)](#equation-choice-problem), the mean-variance portfolio choice problem\nchooses $ w $ to maximize\n\n$$\nE [w ( \\vec r - r_f {\\bf 1})]] - {\\rm var} [ w ( \\vec r - r_f {\\bf 1}) ]\n$$\n\nwhich equals\n\n$$\nw'\\mu - \\frac{\\delta}{2} w' \\Sigma w\n$$\n\nA robust decision maker can be modeled as replacing the mean return\n$ E [w ( \\vec r - r_f {\\bf 1})] $ with the risk-sensitive criterion\n\n$$\n{\\sf T} [w ( \\vec r - r_f {\\bf 1})] = w' \\mu - \\frac{1}{2 \\theta} w' \\Sigma w\n$$\n\nthat comes from replacing the mean $ \\mu $ of $ \\vec r - r\\_f {\\bf 1} $ with the worst-case mean\n\n$$\n\\mu - \\theta^{-1} \\Sigma w\n$$\n\nNotice how the worst-case mean vector depends on the portfolio\n$ w $.\n\nThe operator $ {\\sf T} $ is the indirect utility function that\nemerges from solving a problem in which an agent who chooses\nprobabilities does so in order to minimize the expected utility of a\nmaximizing agent (in our case, the maximizing agent chooses portfolio\nweights $ w $).\n\nThe robust version of the mean-variance portfolio choice problem is then\nto choose a portfolio $ w $ that maximizes\n\n$$\n{\\sf T} [w ( \\vec r - r_f {\\bf 1})] - \\frac{\\delta}{2} w' \\Sigma w\n$$\n\nor\n\n\n<a id='equation-robust-mean-variance'></a>\n$$\nw' (\\mu - \\theta^{-1} \\Sigma w ) - \\frac{\\delta}{2} w' \\Sigma w \\tag{36.7}\n$$\n\nThe minimizer of [(36.7)](#equation-robust-mean-variance) is\n\n$$\nw_{\\rm rob} = \\frac{1}{\\delta + \\gamma } \\Sigma^{-1} \\mu\n$$\n\nwhere $ \\gamma \\equiv \\theta^{-1} $ is sometimes called the\nrisk-sensitivity parameter.\n\nAn increase in the risk-sensitivity parameter $ \\gamma $ shrinks the\nportfolio weights toward zero in the same way that an increase in risk\naversion does.","metadata":{},"id":"e217c0d4"},{"cell_type":"markdown","source":"## Appendix\n\nWe want to illustrate the “folk theorem” that with high or moderate\nfrequency data, it is more difficult to estimate means than variances.\n\nIn order to operationalize this statement, we take two analog\nestimators:\n\n- sample average: $ \\bar X_N = \\frac{1}{N}\\sum_{i=1}^{N} X_i $  \n- sample variance:\n  $ S_N = \\frac{1}{N-1}\\sum_{t=1}^{N} (X_i - \\bar X_N)^2 $  \n\n\nto estimate the unconditional mean and unconditional variance of the\nrandom variable $ X $, respectively.\n\nTo measure the “difficulty of estimation”, we use *mean squared error*\n(MSE), that is the average squared difference between the estimator and\nthe true value.\n\nAssuming that the process $ \\{X_i\\} $is ergodic,\nboth analog estimators are known to converge to their true values as the\nsample size $ N $ goes to infinity.\n\nMore precisely\nfor all $ \\varepsilon > 0 $\n\n$$\n\\lim_{N\\to \\infty} \\ \\ P\\left\\{ \\left |\\bar X_N - \\mathbb E X \\right| > \\varepsilon \\right\\} = 0 \\quad \\quad\n$$\n\nand\n\n$$\n\\lim_{N\\to \\infty} \\ \\ P \\left\\{ \\left| S_N - \\mathbb V X \\right| > \\varepsilon \\right\\} = 0\n$$\n\nA necessary condition for these convergence results is that the\nassociated MSEs vanish as $ N $ goes to infinity, or in other words,\n\n$$\n\\text{MSE}(\\bar X_N, \\mathbb E X) = o(1) \\quad \\quad  \\text{and} \\quad \\quad \\text{MSE}(S_N, \\mathbb V X) = o(1)\n$$\n\nEven if the MSEs converge to zero, the associated rates might be\ndifferent. Looking at the limit of the *relative MSE* (as the sample\nsize grows to infinity)\n\n$$\n\\frac{\\text{MSE}(S_N, \\mathbb V X)}{\\text{MSE}(\\bar X_N, \\mathbb E X)} = \\frac{o(1)}{o(1)} \\underset{N \\to \\infty}{\\to} B\n$$\n\ncan inform us about the relative (asymptotic) rates.\n\nWe will show that in general, with dependent data, the limit\n$ B $ depends on the sampling frequency.\n\nIn particular, we find\nthat the rate of convergence of the variance estimator is less sensitive\nto increased sampling frequency than the rate of convergence of the mean\nestimator.\n\nHence, we can expect the relative asymptotic\nrate, $ B $, to get smaller with higher frequency data,\nillustrating that “it is more difficult to estimate means than\nvariances”.\n\nThat is, we need significantly more data to obtain a given\nprecision of the mean estimate than for our variance estimate.","metadata":{},"id":"a4a42a5e"},{"cell_type":"markdown","source":"## Special Case – IID Sample\n\nWe start our analysis with the benchmark case of IID data.\n\nConsider a\nsample of size $ N $ generated by the following IID process,\n\n$$\nX_i \\sim \\mathcal{N}(\\mu, \\sigma^2)\n$$\n\nTaking $ \\bar X_N $ to estimate the mean, the MSE is\n\n$$\n\\text{MSE}(\\bar X_N, \\mu) = \\frac{\\sigma^2}{N}\n$$\n\nTaking $ S_N $ to estimate the variance, the MSE is\n\n$$\n\\text{MSE}(S_N, \\sigma^2) = \\frac{2\\sigma^4}{N-1}\n$$\n\nBoth estimators are unbiased and hence the MSEs reflect the\ncorresponding variances of the estimators.\n\nFurthermore, both MSEs are\n$ o(1) $ with a (multiplicative) factor of difference in their rates\nof convergence:\n\n$$\n\\frac{\\text{MSE}(S_N, \\sigma^2)}{\\text{MSE}(\\bar X_N, \\mu)} = \\frac{N2\\sigma^2}{N-1} \\quad \\underset{N \\to \\infty}{\\to} \\quad 2\\sigma^2\n$$\n\nWe are interested in how this (asymptotic) relative rate of convergence\nchanges as increasing sampling frequency puts dependence into the data.","metadata":{},"id":"cf0305ee"},{"cell_type":"markdown","source":"## Dependence and Sampling Frequency\n\nTo investigate how sampling frequency affects relative rates of\nconvergence, we assume that the data are generated by a mean-reverting\ncontinuous time process of the form\n\n$$\ndX_t = -\\kappa (X_t -\\mu)dt + \\sigma dW_t\\quad\\quad\n$$\n\nwhere $ \\mu $is the unconditional mean, $ \\kappa > 0 $ is a\npersistence parameter, and $ \\{W_t\\} $ is a standardized Brownian\nmotion.\n\nObservations arising from this system in particular discrete periods\n$ \\mathcal T(h) \\equiv \\{nh : n \\in \\mathbb Z \\} $with$ h>0 $\ncan be described by the following process\n\n$$\nX_{t+1} = (1 - \\exp(-\\kappa h))\\mu + \\exp(-\\kappa h)X_t + \\epsilon_{t, h}\n$$\n\nwhere\n\n$$\n\\epsilon_{t, h} \\sim \\mathcal{N}(0, \\Sigma_h) \\quad \\text{with}\\quad \\Sigma_h = \\frac{\\sigma^2(1-\\exp(-2\\kappa h))}{2\\kappa}\n$$\n\nWe call $ h $ the *frequency* parameter, whereas $ n $\nrepresents the number of *lags* between observations.\n\nHence, the effective distance between two observations $ X_t $ and\n$ X_{t+n} $ in the discrete time notation is equal\nto $ h\\cdot n $ in terms of the underlying continuous time process.\n\nStraightforward calculations show that the autocorrelation function for\nthe stochastic process $ \\{X_{t}\\}_{t\\in \\mathcal T(h)} $ is\n\n$$\n\\Gamma_h(n) \\equiv \\text{corr}(X_{t + h n}, X_t) = \\exp(-\\kappa h n)\n$$\n\nand the auto-covariance function is\n\n$$\n\\gamma_h(n) \\equiv \\text{cov}(X_{t + h n}, X_t) = \\frac{\\exp(-\\kappa h n)\\sigma^2}{2\\kappa} .\n$$\n\nIt follows that if $ n=0 $, the unconditional variance is given\nby $ \\gamma_h(0) = \\frac{\\sigma^2}{2\\kappa} $ irrespective of the\nsampling frequency.\n\nThe following figure illustrates how the dependence between the\nobservations is related to the sampling frequency\n\n- For any given $ h $, the autocorrelation converges to zero as we increase the distance – $ n $– between the observations. This represents the “weak dependence” of the $ X $ process.  \n- Moreover, for a fixed lag length, $ n $, the dependence vanishes as the sampling frequency goes to infinity. In fact, letting $ h $ go to $ \\infty $ gives back the case of IID data.  ","metadata":{},"id":"d91c2311"},{"cell_type":"code","source":"μ = .0\nκ = .1\nσ = .5\nvar_uncond = σ**2 / (2 * κ)\n\nn_grid = np.linspace(0, 40, 100)\nautocorr_h1 = np.exp(-κ * n_grid * 1)\nautocorr_h2 = np.exp(-κ * n_grid * 2)\nautocorr_h5 = np.exp(-κ * n_grid * 5)\nautocorr_h1000 = np.exp(-κ * n_grid * 1e8)\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.plot(n_grid, autocorr_h1, label=r'$h=1$', c='darkblue', lw=2)\nax.plot(n_grid, autocorr_h2, label=r'$h=2$', c='darkred', lw=2)\nax.plot(n_grid, autocorr_h5, label=r'$h=5$', c='orange', lw=2)\nax.plot(n_grid, autocorr_h1000, label=r'\"$h=\\infty$\"', c='darkgreen', lw=2)\nax.legend()\nax.grid()\nax.set(title=r'Autocorrelation functions, $\\Gamma_h(n)$',\n       xlabel=r'Lags between observations, $n$')\nplt.show()","metadata":{"hide-output":false},"execution_count":null,"outputs":[],"id":"c4a88087"},{"cell_type":"markdown","source":"## Frequency and the Mean Estimator\n\nConsider again the AR(1) process generated by discrete sampling with\nfrequency $ h $. Assume that we have a sample of size $ N $ and\nwe would like to estimate the unconditional mean – in our case the true\nmean is $ \\mu $.\n\nAgain, the sample average is an unbiased estimator of the unconditional\nmean\n\n$$\n\\mathbb{E}[\\bar X_N] = \\frac{1}{N}\\sum_{i = 1}^N \\mathbb{E}[X_i] = \\mathbb{E}[X_0] = \\mu\n$$\n\nThe variance of the sample mean is given by\n\n$$\n\\begin{aligned}\n\\mathbb{V}\\left(\\bar X_N\\right) &= \\mathbb{V}\\left(\\frac{1}{N}\\sum_{i = 1}^N X_i\\right) \\\\\n&= \\frac{1}{N^2} \\left(\\sum_{i = 1}^N \\mathbb{V}(X_i) + 2 \\sum_{i = 1}^{N-1} \\sum_{s = i+1}^N \\text{cov}(X_i, X_s) \\right) \\\\\n&= \\frac{1}{N^2} \\left( N \\gamma(0) + 2 \\sum_{i=1}^{N-1} i \\cdot \\gamma\\left(h\\cdot (N - i)\\right) \\right) \\\\\n&= \\frac{1}{N^2} \\left( N \\frac{\\sigma^2}{2\\kappa} + 2 \\sum_{i=1}^{N-1} i \\cdot \\exp(-\\kappa h (N - i)) \\frac{\\sigma^2}{2\\kappa} \\right)\n\\end{aligned}\n$$\n\nIt is explicit in the above equation that time dependence in the data\ninflates the variance of the mean estimator through the covariance\nterms.\n\nMoreover, as we can see, a higher sampling frequency—smaller\n$ h $—makes all the covariance terms larger, everything else being\nfixed.\n\nThis implies a relatively slower rate of convergence of the\nsample average for high-frequency data.\n\nIntuitively, stronger dependence across observations for high-frequency data reduces the\n“information content” of each observation relative to the IID case.\n\nWe can upper bound the variance term in the following way\n\n$$\n\\begin{aligned}\n\\mathbb{V}(\\bar X_N) &= \\frac{1}{N^2} \\left( N \\sigma^2 + 2 \\sum_{i=1}^{N-1} i \\cdot \\exp(-\\kappa h (N - i)) \\sigma^2 \\right) \\\\\n&\\leq \\frac{\\sigma^2}{2\\kappa N} \\left(1 + 2 \\sum_{i=1}^{N-1} \\cdot \\exp(-\\kappa h (i)) \\right) \\\\\n&= \\underbrace{\\frac{\\sigma^2}{2\\kappa N}}_{\\text{IID  case}} \\left(1 + 2 \\frac{1 - \\exp(-\\kappa h)^{N-1}}{1 - \\exp(-\\kappa h)} \\right)\n\\end{aligned}\n$$\n\nAsymptotically, the term $ \\exp(-\\kappa h)^{N-1} $ vanishes and the\ndependence in the data inflates the benchmark IID variance by a factor\nof\n\n$$\n\\left(1 + 2 \\frac{1}{1 - \\exp(-\\kappa h)} \\right)\n$$\n\nThis long run factor is larger the higher is the frequency (the smaller\nis $ h $).\n\nTherefore, we expect the asymptotic relative MSEs, $ B $, to change\nwith time-dependent data. We just saw that the mean estimator’s rate is\nroughly changing by a factor of\n\n$$\n\\left(1 + 2 \\frac{1}{1 - \\exp(-\\kappa h)} \\right)\n$$\n\nUnfortunately, the variance estimator’s MSE is harder to derive.\n\nNonetheless, we can approximate it by using (large sample) simulations,\nthus getting an idea about how the asymptotic relative MSEs changes in\nthe sampling frequency $ h $ relative to the IID case that we\ncompute in closed form.","metadata":{},"id":"8515e57b"},{"cell_type":"code","source":"def sample_generator(h, N, M):\n    ϕ = (1 - np.exp(-κ * h)) * μ\n    ρ = np.exp(-κ * h)\n    s = σ**2 * (1 - np.exp(-2 * κ * h)) / (2 * κ)\n\n    mean_uncond = μ\n    std_uncond = np.sqrt(σ**2 / (2 * κ))\n\n    ε_path = stat.norm(0, np.sqrt(s)).rvs((M, N))\n\n    y_path = np.zeros((M, N + 1))\n    y_path[:, 0] = stat.norm(mean_uncond, std_uncond).rvs(M)\n\n    for i in range(N):\n        y_path[:, i + 1] = ϕ + ρ * y_path[:, i] + ε_path[:, i]\n\n    return y_path","metadata":{"hide-output":false},"execution_count":null,"outputs":[],"id":"77208081"},{"cell_type":"code","source":"# Generate large sample for different frequencies\nN_app, M_app = 1000, 30000        # Sample size, number of simulations\nh_grid = np.linspace(.1, 80, 30)\n\nvar_est_store = []\nmean_est_store = []\nlabels = []\n\nfor h in h_grid:\n    labels.append(h)\n    sample = sample_generator(h, N_app, M_app)\n    mean_est_store.append(np.mean(sample, 1))\n    var_est_store.append(np.var(sample, 1))\n\nvar_est_store = np.array(var_est_store)\nmean_est_store = np.array(mean_est_store)\n\n# Save mse of estimators\nmse_mean = np.var(mean_est_store, 1) + (np.mean(mean_est_store, 1) - μ)**2\nmse_var = np.var(var_est_store, 1) \\\n          + (np.mean(var_est_store, 1) - var_uncond)**2\n\nbenchmark_rate = 2 * var_uncond       # IID case\n\n# Relative MSE for large samples\nrate_h = mse_var / mse_mean\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot(h_grid, rate_h, c='darkblue', lw=2,\n        label=r'large sample relative MSE, $B(h)$')\nax.axhline(benchmark_rate, c='k', ls='--', label=r'IID benchmark')\nax.set_title('Relative MSE for large samples as a function of sampling \\\n              frequency \\n MSE($S_N$) relative to MSE($\\\\bar X_N$)')\nax.set_xlabel('Sampling frequency, $h$')\nax.legend()\nplt.show()","metadata":{"hide-output":false},"execution_count":null,"outputs":[],"id":"bf419eb0"},{"cell_type":"markdown","source":"The above figure illustrates the relationship between the asymptotic\nrelative MSEs and the sampling frequency\n\n- We can see that with low-frequency data – large values of $ h $\n  – the ratio of asymptotic rates approaches the IID case.  \n- As $ h $ gets smaller – the higher the frequency – the relative\n  performance of the variance estimator is better in the sense that the\n  ratio of asymptotic rates gets smaller. That is, as the time\n  dependence gets more pronounced, the rate of convergence of the mean\n  estimator’s MSE deteriorates more than that of the variance\n  estimator.  ","metadata":{},"id":"ffdf3f79"}]}